---
title: "Attention Is Still the Bottleneck"
slug: "attention-is-still-the-bottleneck"
date: "2025-10-15"
description: "Agents multiply options. They make it easier to start. They do not automatically make it easier to finish. The finishing still requires me."
tags:
  - Focus
  - Agentic engineering
  - Productivity
---

I want to talk about attention, because every time I use agent tools I notice the same constraint. The bottleneck is rarely the model. The bottleneck is whether I can hold a single thread long enough to finish something difficult.

Cal Newport has been making this point for years. Deep work, sustained focus on cognitively demanding tasks, is becoming rarer at exactly the moment it is becoming more valuable. I have found that to be true in my own work, especially when the pace of tools increases.

It is tempting to believe that agents will solve focus for us. You hand off the hard part and the machine does it. In practice, agents often multiply options. They give you ten plausible branches to chase. They make it easier to start. They do not automatically make it easier to finish.

The finishing still requires me.

There is research that helps explain why this is hard. Sophie Leroy described attention residue, the idea that part of your mind stays attached to a previous task when you switch. Gloria Mark has studied how often people get interrupted and what it costs to recover focus. I do not need the exact numbers to take the point seriously. Task switching is not free.

I feel it quickly when I work.

If I bounce between messages, meetings, news, and code, my day looks full and my output gets thinner. I get less careful. I get more impatient. I accept partial solutions. Then I spend the next day paying for that impatience in debugging and rework.

Agentic engineering can make this better or worse.

If I use an assistant to reduce friction inside a focused session, it feels like leverage. If I use it as an excuse to chase every new thought the model suggests, it becomes a structured form of distraction. The output increases and the clarity decreases.

What has worked best for me is choosing a single primary thread for a work session and defending it.

I pick a milestone that is concrete and verifiable. Something like an endpoint returning correct data locally. A model training run that is reproducible. A deployment pipeline that is green. Then I use the agent to help inside that thread. Error explanations. A draft test. A refactor plan that keeps behavior stable. The goal is momentum without drift.

I also try to keep my environment ready to run. If the code does not run locally, everything becomes abstract. If I can run it, I can validate. That validation loop keeps me grounded.

When the conversation becomes noisy, I reset. New thread. Smaller scope. Cleaner prompt. I treat it like moving back to first principles, not like a failure.

None of this is about being rigid. It is about respecting cognitive limits, including my own.

The part I find interesting is that attention management is starting to feel like a technical competency. Not in a motivational way. In a practical way. Can I keep my mental model stable long enough to verify a change. Can I stay patient enough to test the edges. Can I finish.

If you have a system for protecting focus while building with agents, I would like to learn from it. I am collecting practices that actually work, not slogans.

---
title: "What consulting taught me about building AI responsibly"
slug: "consulting-and-responsible-ai"
date: "2024-09-20"
description: "In consulting, your model isn't the deliverable. The deliverable is a system people can trust, and that takes more than good code."
tags:
  - Consulting
  - Responsible AI
  - Governance
---

In consulting, your model isn't the deliverable. The deliverable is a system people can trust.

I didn't fully understand this when I started. Like a lot of people who come into AI from a technical background, I thought the hard part was the model: the embeddings, the clustering, the evaluation metrics. Get the model right, and everything else follows.

It doesn't.

## The day-after test

The most important question in consulting isn't "does this work?" It's "will this still work the day after we leave?"

That question changes everything about how you build.

Documentation isn't optional. It's a core deliverable. If the client can't understand what the system does and why, they can't maintain it, and they can't trust it. Governance isn't bureaucracy. It's the framework that tells people who can change what, who approves updates, and who's accountable when something goes wrong. And stakeholder communication isn't a soft skill. It's the difference between a system that gets adopted and one that gets shelved.

## Accessibility is a requirement

Working in federal consulting taught me to treat **accessibility and Section 508 compliance** as engineering requirements. Every dashboard, every report, every interface needs to work for everyone who needs to use it.

This isn't just about legal compliance, though it is that. It's about building systems that actually serve their users. If a program officer with low vision can't read your dashboard, your dashboard doesn't work. That's not an edge case. That's a failure.

## Change management is delivery

The most technically impressive system I've built is also the one that required the most non-technical work: training sessions, office hours, documentation, stakeholder check-ins, and the slow, patient work of helping people trust a new tool.

This is the part of AI work that doesn't show up in blog posts or conference talks. But it's where adoption lives or dies.

## What "responsible AI" means in practice

In my experience, responsible AI isn't a philosophy. It's a set of concrete questions. Can the user understand why the system made a recommendation? Can they override it? Is there an audit trail? Has the system been tested for bias? Can someone who didn't build it maintain it?

If the answer to any of those is no, there's work to do. None of these are flashy. All of them are necessary. Consulting taught me that governance and adoption aren't separate from engineering. They're part of it.

---
title: "What consulting taught me about building AI responsibly"
slug: "consulting-and-responsible-ai"
date: "2024-09-20"
description: "In consulting, your model isn't the deliverable. The deliverable is a system people can trust — and that requires more than good code."
tags:
  - Consulting
  - Responsible AI
  - Governance
---

In consulting, your model isn't the deliverable. The deliverable is a system people can trust.

I didn't fully understand this when I started. Like a lot of people who come into AI from a technical background, I thought the hard part was the model — the embeddings, the clustering, the evaluation metrics. Get the model right, and everything else follows.

It doesn't.

## The day-after test

The most important question in consulting isn't "does this work?" — it's "will this still work the day after we leave?"

That question changes everything about how you build:

- **Documentation** isn't optional. It's a core deliverable. If the client can't understand what the system does and why, they can't maintain it, and they can't trust it.
- **Governance** isn't bureaucracy. It's the framework that tells people who can change what, who approves updates, and who's accountable when something goes wrong.
- **Stakeholder alignment** isn't a soft skill. It's the difference between a system that gets adopted and a system that gets shelved.

## Accessibility is a requirement, not a feature

Working in federal consulting taught me to treat **accessibility and Section 508 compliance** as engineering requirements, not nice-to-haves. Every dashboard, every report, every interface needs to work for everyone who needs to use it.

This isn't just about legal compliance (though it is that). It's about building systems that actually serve their users. If a program officer with low vision can't read your dashboard, your dashboard doesn't work.

## Change management is delivery

The most technically impressive system I've built is also the one that required the most non-technical work: training sessions, office hours, documentation, stakeholder check-ins, and the slow, patient work of helping people trust a new tool.

This is the part of AI work that doesn't show up in blog posts or conference talks. But it's where adoption lives or dies.

## What "responsible AI" means in practice

In my experience, responsible AI isn't a philosophy — it's a checklist:

- **Can the user understand why the system made a recommendation?** If not, add transparency.
- **Can the user override the recommendation?** If not, rethink the design.
- **Is there an audit trail?** If not, build one.
- **Has the system been tested for bias and fairness?** If not, that's a gap.
- **Can the system be maintained by someone who didn't build it?** If not, your documentation isn't done.

None of these are flashy. All of them are necessary. Consulting taught me that governance and adoption aren't separate from engineering — they're part of it.

---
title: "Telemetry dashboards: Measuring what matters for tool adoption"
slug: "nsf-telemetry"
client: "National Science Foundation (sanitized)"
timeframe: "Recent (sanitized)"
role: "Analytics lead"
stack:
  - Telemetry
  - Dashboards
  - Anonymous usage tracking
  - Python
tags:
  - Adoption
  - Telemetry
  - Governance
  - Analytics
featured: false
executiveSummary: "Built anonymous telemetry dashboards to monitor real-world adoption of every tool in the analytics suite — turning usage data into actionable insights about what's working."
builderSummary: "Designed and deployed anonymous usage collection across multiple tools with dashboards that tracked adoption patterns, feature engagement, and workflow bottlenecks."
---

## The problem

Building tools is only half the job. Understanding whether people actually use them — and *how* they use them — is what separates successful delivery from shelfware. Across the suite of analytics and decision-support tools I built, there was no systematic way to measure adoption, identify underused features, or detect workflow bottlenecks.

## What I built

I designed and deployed **telemetry dashboards** that monitor and collect **anonymous usage data** across each tool in the suite. The system tracks:

- **Adoption metrics**: who is using which tools, how frequently, and for how long
- **Feature engagement**: which capabilities within each tool are being used, and which are being ignored
- **Workflow patterns**: how users move through multi-step processes, where they stall, and where they abandon
- **Trend analysis**: adoption trajectories over time — are tools gaining traction or falling off?

### Privacy-first design

All telemetry is **anonymous by design**. The system collects behavioral patterns without identifying individual users, ensuring that:
- Staff feel comfortable knowing their individual usage isn't being monitored
- Leadership gets the aggregate insights they need for resource allocation decisions
- The data can be shared openly in working groups and reports without privacy concerns

## Impact

- **Evidence-based iteration**: telemetry data directly informed which features to improve, simplify, or retire
- **Adoption visibility**: leadership could see which tools were delivering value and which needed enablement support
- **Early warning**: declining usage patterns surfaced issues before they became entrenched — enabling intervention while habits were still forming
- **Cross-tool insights**: comparing adoption patterns across the suite revealed which design approaches drove engagement

*Details are sanitized to protect agency-specific information.*

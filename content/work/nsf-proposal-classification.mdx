---
title: "Proposal classification: BERTopic + Bayesian optimization"
slug: "nsf-proposal-classification"
client: "National Science Foundation (sanitized)"
timeframe: "Recent (sanitized)"
role: "Applied data scientist / ML engineer"
stack:
  - Python
  - BERTopic
  - UMAP
  - HDBSCAN
  - Sentence Transformers
  - Optuna
  - Plotly
  - Pandas
  - scikit-learn
tags:
  - NLP
  - Topic modeling
  - Unsupervised learning
  - BERTopic
  - Optimization
  - AI/ML
featured: false
image: "/images/projects/optuna-classification.png"
executiveSummary: "Clustered 7,000+ research proposals into 70+ themes using BERTopic with Bayesian optimization across 100+ trials. 40% improvement in clustering quality over baseline. Ambiguous proposals flagged for human review rather than forced into poor-fit clusters."
builderSummary: "BERTopic pipeline with UMAP + HDBSCAN + transformer embeddings. Optuna Bayesian optimization across 100+ trials. Plotly dashboards for topic exploration and organizational alignment."
---

## The problem

Research funding agencies receive thousands of proposals each review cycle. Before any of those can be evaluated, they need to be grouped into thematically coherent clusters and assigned to panels with the right domain expertise. Doing this manually is slow, inconsistent, and heavily dependent on institutional knowledge that walks out the door when experienced staff rotate.

I needed to build a system that could discover the natural thematic structure in a corpus of **7,000+ research proposals** and surface those groupings for human review.

## How I approached it

The classification system uses **BERTopic**, a modular topic modeling framework that chains together three techniques. Transformer-based embeddings (Sentence Transformers) convert proposal abstracts into dense 768-dimensional vectors that capture semantic meaning far beyond keyword matching. UMAP reduces those high-dimensional embeddings into a space where clustering is computationally tractable while preserving local and global structure. And HDBSCAN discovers natural groupings without requiring you to pre-specify the number of topics, which matters when the thematic landscape of a proposal corpus isn't known in advance.

The result is interpretable topic clusters with associated keywords, representative documents, and confidence scores.

## Tuning with Bayesian optimization

Topic modeling involves dozens of interconnected parameters (UMAP neighbors, minimum distances, HDBSCAN cluster sizes, embedding model selection) and the interactions between them are non-obvious. Manual tuning would have taken weeks.

I used **Optuna** to run Bayesian optimization across **100+ trials**, optimizing for topic coherence (are the themes internally consistent?), coverage (what percentage of proposals get meaningful assignments?), and granularity (are topics specific enough for panel formation?). That automated search **improved clustering quality by 40%** over baseline parameters, a gain I'm confident I wouldn't have found through manual iteration.

## What the model found

The optimized model discovered **70+ distinct research themes** across the corpus. About **70% of proposals** mapped cleanly to coherent thematic clusters. The remaining **30%** didn't fit clearly into any single topic and were flagged for manual review rather than forced into poor-fit clusters.

That 30% noise ratio is intentional. Forcing every proposal into a cluster inflates apparent accuracy while degrading actual usefulness. Flagging ambiguous cases for human judgment is the honest approach, and it's more useful to the people building panels.

## Dashboards and analysis

I built interactive visualizations in **Plotly** so stakeholders could explore the results: topic maps showing the semantic landscape of the corpus, organizational alignment views mapping topics to institutional priorities, quality dashboards tracking clustering performance across optimization trials, and drilldowns into representative proposals and keyword distributions within each theme.

## The data engineering underneath

The ML pipeline needed solid data engineering to hold together. That included text preprocessing and abstract extraction, batch embedding generation for large datasets, caching and checkpointing for long-running optimization runs, and export pipelines connecting classification results to downstream panel formation workflows.

## What it changed

Proposal clustering went from a weeks-long manual effort to an hours-long guided review. The Bayesian optimization discovered parameter combinations I wouldn't have found by hand. The thematic groupings directly informed reviewer-proposal matching, and the pipeline scales with submission volume without requiring proportional manual effort.

*Details are sanitized to protect agency-specific information.*
